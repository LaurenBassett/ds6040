{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lauren Bassett\n",
    "# DS 6040\n",
    "# Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import BernoulliNB \n",
    "from sklearn.metrics import accuracy_score\n",
    "import scipy.stats as st\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1\n",
    "\n",
    "You are a data scientist and are choosing between three approaches, A, B,\n",
    "and C, to a problem. With approach A you will spend a total of 4 days coding\n",
    "and running an algorithm and it will not produce useful results. With approach\n",
    "B you will spend a total of 3 days coding and running an algorithm and it will\n",
    "not produce useful results. With approach C you will spend 1 day coding and\n",
    "running an algorithm and it will produce useful results. You are equally likely to\n",
    "choose among unselected options. What is the expected time in days for you to\n",
    "obtain the results you are looking for, if you continue to select an unselected\n",
    "option when you do not obtain useful results? What is the variance on this time?\n",
    "\n",
    "Visual of Probabilities:\n",
    "![](HW1Q1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The expected value is: 4.5 days\n",
      "The total variance is: 8.25 days\n"
     ]
    }
   ],
   "source": [
    "probabilities = [1/6,1/6,1/6,1/6,1/3]\n",
    "days = [8,5,8,4,1]\n",
    "\n",
    "total_odds = []\n",
    "all_variance = []\n",
    "for i in range(len(probabilities)):\n",
    "    total_odds.append(probabilities[i] * days[i])\n",
    "\n",
    "expected_value = sum(total_odds)\n",
    "\n",
    "for j in range(len(total_odds)):\n",
    "    all_variance.append(probabilities[j] * ((days[j] - expected_value)**2))\n",
    "\n",
    "total_variance = sum(all_variance)\n",
    "\n",
    "print(\"The expected value is:\", expected_value, \"days\")\n",
    "print(\"The total variance is:\", total_variance, \"days\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2\n",
    "\n",
    "Suppose if it is sunny or not in Charlottesville depends on the weather of\n",
    "the last three days. Show how this can be modeled as a Markov chain.\n",
    "\n",
    "\n",
    "There are 8 Possibilities, which are shown below, as well as the possible transition matrix. The values that would have a probability (the transition is possible) are colored in blue, all other probabilities are noted with a zero.  Based on this transition Matrix, modeling this weather pattern would be possible in a Markov Chain. \n",
    "\n",
    "![](MarkovTables.jpg)\n",
    "\n",
    "The Node Visual of the Markov Chain would be represented like this:\n",
    "![](MarkovNode.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\n",
    "Assume a Gaussian distribution for observations, Xi, i = 1, . . . , N with\n",
    "unknown mean, M, and known variance 5. Suppose the prior for M is Gaussian\n",
    "with variance 10. How large a random sample must be taken (i.e., what is the\n",
    "minimum value for N) to specify an interval having unit length of 1 such that\n",
    "the probability that M lies in this interval is 0.95?\n",
    "\n",
    "![](HW1Q3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4\n",
    "You have started an online business selling books that are of interest to\n",
    "your customers. A publisher has just given you a large book with photos from\n",
    "famous 20th century photographers. You think this book will appeal to people\n",
    "who have bought art books, history books and coffee table books. In an initial\n",
    "offering of the new book you collect data on purchases of the new book and\n",
    "combine these data with data from the past purchases (see ArtHistBooks.csv).\n",
    "Use Bayesian analysis to give the posterior probabilities for purchases of art\n",
    "books, history books and coffee table books, as well as, the separate probabilities\n",
    "for purchases of the new book given each possible combination of prior\n",
    "purchases of art books, history books and coffee table books. Do this by first\n",
    "using beta priors with values of the hyperparameters that represent lack of prior\n",
    "information. Then compute these probabilities again with beta priors that show\n",
    "strong weighting for low likelihood of a book purchase. Compare your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArtBooks  HistoryBooks  TableBooks  Purchase\n",
       "0         1             0           0           241\n",
       "          0             0           0           189\n",
       "                        1           0           131\n",
       "          1             1           0           107\n",
       "1         1             0           0            84\n",
       "          0             0           0            64\n",
       "                        1           0            50\n",
       "          1             1           0            45\n",
       "                                    1            26\n",
       "                        0           1            16\n",
       "0         1             1           1            14\n",
       "1         0             0           1            12\n",
       "0         1             0           1            10\n",
       "          0             0           1             4\n",
       "1         0             1           1             4\n",
       "0         0             1           1             3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_data = pd.read_csv('ArtHistBooks.csv')\n",
    "#probably not the most efficient way to do this but it gets the job done/ \n",
    "art_data.ArtBooks= art_data.ArtBooks.apply(lambda x: 1 if x!=0 else 0)\n",
    "art_data.HistoryBooks= art_data.HistoryBooks.apply(lambda x: 1 if x!=0 else 0)\n",
    "art_data.TableBooks = art_data.TableBooks.apply(lambda x: 1 if x!=0 else 0)\n",
    "art_data.Purchase= art_data.Purchase.apply(lambda x: 1 if x!=0 else 0)\n",
    "#if value is non zero, then value is 1:\n",
    "#art_data.any(lambda x: x==1 if x!=0 else 0)\n",
    "art_data.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Did they buy an art book P1\n",
    "#Did they buy a history book\n",
    "#Did they buy a Table Book \n",
    "#Did they buy a new book? \n",
    "params = np.arange(0,1,0.01)\n",
    "grouped_art = art_data.groupby([\"ArtBooks\", \"HistoryBooks\", \"TableBooks\", \"Purchase\"]).size().unstack().reset_index()\n",
    "grouped_art['N'] = (grouped_art[1]+grouped_art[0])\n",
    "grouped_art['K'] = grouped_art[1]\n",
    "grouped_art['Probability'] = grouped_art['K']/(grouped_art['N'])\n",
    "grouped_art['Alpha1']  = 1 + grouped_art['K']\n",
    "grouped_art['Beta1'] =1 + grouped_art['N']-grouped_art['K']\n",
    "grouped_art['Uninformed Mean'] = grouped_art['Alpha1'] / (grouped_art['Alpha1'] + grouped_art['Beta1'])\n",
    "grouped_art['Alpha2']  = 1 + grouped_art['K']\n",
    "grouped_art['Beta2'] =20 + grouped_art['N']-grouped_art['K']\n",
    "grouped_art['Low Bias Mean'] = grouped_art['Alpha2'] / (grouped_art['Alpha2'] + grouped_art['Beta2'])\n",
    "#art_data.Purchase.value_counts()\n",
    "\n",
    "#art_data.Purchase.value_counts()\n",
    "#art_data.Purchase.value_counts()\n",
    "#P_Purchased = 89/1000\n",
    "#P_NotPurchased = 911/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Purchase</th>\n",
       "      <th>ArtBooks</th>\n",
       "      <th>HistoryBooks</th>\n",
       "      <th>TableBooks</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>N</th>\n",
       "      <th>K</th>\n",
       "      <th>Probability</th>\n",
       "      <th>Alpha1</th>\n",
       "      <th>Beta1</th>\n",
       "      <th>Uninformed Mean</th>\n",
       "      <th>Alpha2</th>\n",
       "      <th>Beta2</th>\n",
       "      <th>Low Bias Mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>189</td>\n",
       "      <td>4</td>\n",
       "      <td>193</td>\n",
       "      <td>4</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>5</td>\n",
       "      <td>190</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>5</td>\n",
       "      <td>209</td>\n",
       "      <td>0.023364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>131</td>\n",
       "      <td>3</td>\n",
       "      <td>134</td>\n",
       "      <td>3</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>4</td>\n",
       "      <td>132</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>4</td>\n",
       "      <td>151</td>\n",
       "      <td>0.025806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>241</td>\n",
       "      <td>10</td>\n",
       "      <td>251</td>\n",
       "      <td>10</td>\n",
       "      <td>0.039841</td>\n",
       "      <td>11</td>\n",
       "      <td>242</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>11</td>\n",
       "      <td>261</td>\n",
       "      <td>0.040441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>14</td>\n",
       "      <td>121</td>\n",
       "      <td>14</td>\n",
       "      <td>0.115702</td>\n",
       "      <td>15</td>\n",
       "      <td>108</td>\n",
       "      <td>0.121951</td>\n",
       "      <td>15</td>\n",
       "      <td>127</td>\n",
       "      <td>0.105634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>76</td>\n",
       "      <td>12</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>13</td>\n",
       "      <td>65</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>13</td>\n",
       "      <td>84</td>\n",
       "      <td>0.134021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>4</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>0.089286</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>16</td>\n",
       "      <td>100</td>\n",
       "      <td>16</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>17</td>\n",
       "      <td>85</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>17</td>\n",
       "      <td>104</td>\n",
       "      <td>0.140496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>26</td>\n",
       "      <td>71</td>\n",
       "      <td>26</td>\n",
       "      <td>0.366197</td>\n",
       "      <td>27</td>\n",
       "      <td>46</td>\n",
       "      <td>0.369863</td>\n",
       "      <td>27</td>\n",
       "      <td>65</td>\n",
       "      <td>0.293478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Purchase  ArtBooks  HistoryBooks  TableBooks    0   1    N   K  Probability  \\\n",
       "0                0             0           0  189   4  193   4     0.020725   \n",
       "1                0             0           1  131   3  134   3     0.022388   \n",
       "2                0             1           0  241  10  251  10     0.039841   \n",
       "3                0             1           1  107  14  121  14     0.115702   \n",
       "4                1             0           0   64  12   76  12     0.157895   \n",
       "5                1             0           1   50   4   54   4     0.074074   \n",
       "6                1             1           0   84  16  100  16     0.160000   \n",
       "7                1             1           1   45  26   71  26     0.366197   \n",
       "\n",
       "Purchase  Alpha1  Beta1  Uninformed Mean  Alpha2  Beta2  Low Bias Mean  \n",
       "0              5    190         0.025641       5    209       0.023364  \n",
       "1              4    132         0.029412       4    151       0.025806  \n",
       "2             11    242         0.043478      11    261       0.040441  \n",
       "3             15    108         0.121951      15    127       0.105634  \n",
       "4             13     65         0.166667      13     84       0.134021  \n",
       "5              5     51         0.089286       5     70       0.066667  \n",
       "6             17     85         0.166667      17    104       0.140496  \n",
       "7             27     46         0.369863      27     65       0.293478  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_art"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results:\n",
    "\n",
    "The Uninformed Mean never outperformed the low bias mean. The low bias mean was always less than the uninformed mean. \n",
    "\n",
    "The Probability was lower than the mean for purchase combinations 0, 1, and 2.\n",
    "The Uninformed mean was higher than the probability for puchase combinations 0, 1,2,3,4,5,6, and 7 (always higher!)\n",
    "\n",
    "The uninformed mean did not perform as well compared to the low bias mean. \n",
    "I believe this shows that it is not a perfect predictor on who will buy the new book given their previous purchases. There is a low level of bias in this distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(15) The data set CHDdata.csv contains cases of coronary heart disease (CHD)\n",
    "and variables associated with the patient’s condition: systolic\n",
    "1\n",
    "Donald E. Brown DS 6014\n",
    "blood pressure, yearly tobacco use (in kg), low density lipoprotein (ldl),\n",
    "adiposity, family history (0 or 1), type A personality score (typea), obesity (body\n",
    "mass index), alcohol use, age, and the diagnosis of CHD (0 or 1).\n",
    "Perform a Bayesian analysis of these data that finds the posterior marginal\n",
    "probability distributions for the means for the data of patients with and without\n",
    "CHD. You should first standard scale (subtract the mean and divide by the\n",
    "standard deviation) all the numeric variables (remove family history and do not\n",
    "scale CHD). Then separate the data into two sets, one for patients with CHD\n",
    "and one for patients without CHD.\n",
    "Your priors for both groups should assume means of 0 for all variables and a\n",
    "correlation of 0 between all pairs of variables. You should assume all variances\n",
    "for the variables are 1. Use a prior alpha equal to one plus the number of\n",
    "predictor variables. Compute and compare the Bayesian estimates for the posterior means for each group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbp\n",
      "tobacco\n",
      "ldl\n",
      "adiposity\n",
      "typea\n",
      "obesity\n",
      "alcohol\n",
      "age\n"
     ]
    }
   ],
   "source": [
    "## Preparing the Data\n",
    "\n",
    "CHD = pd.read_csv('CHDdata.csv')\n",
    "#Remove Family History\n",
    "CHD = CHD.drop(columns='famhist')\n",
    "#You should first standard scale all numeric variables\n",
    "for key in CHD.keys()[0:8]:\n",
    "    print(key)\n",
    "    average = np.mean(CHD[key])\n",
    "    std = np.std(CHD[key])\n",
    "    CHD[key] = (CHD[key]-average)/std\n",
    "\n",
    "#Then, separate the data into two sets, one with CHD, one without\n",
    "CHD_POS = CHD[CHD.chd == 1]\n",
    "CHD_NEG = CHD[CHD.chd == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when we use conjugate priors, we only need to use Python like a simple calculator\n",
    "def get_estimate(key):\n",
    "    n = len(key)\n",
    "    xbar = np.average(key)\n",
    "    tau0 = 100 # chosen hyperparameter\n",
    "    mu0 = 0  # chosen hyperparameter\n",
    "    tau = 1 # assumed known\n",
    "    # \"getting\" the posterior is simply invoking the formula\n",
    "    posterior_mean = xbar*(n*tau/(tau0 + n*tau )) + mu0*(tau0/(tau0 + n*tau ))\n",
    "    posterior_precision = tau0 + n*tau \n",
    "    posterior_variance = 1/posterior_precision\n",
    "    return(posterior_mean, posterior_variance)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>response</th>\n",
       "      <th>posterior_mean</th>\n",
       "      <th>posterior_variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sbp</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.162627</td>\n",
       "      <td>0.003846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sbp</td>\n",
       "      <td>NEG</td>\n",
       "      <td>-0.105181</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tobacco</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.253397</td>\n",
       "      <td>0.003846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tobacco</td>\n",
       "      <td>NEG</td>\n",
       "      <td>-0.163889</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ldl</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.222399</td>\n",
       "      <td>0.003846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ldl</td>\n",
       "      <td>NEG</td>\n",
       "      <td>-0.143840</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adiposity</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.214848</td>\n",
       "      <td>0.003846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adiposity</td>\n",
       "      <td>NEG</td>\n",
       "      <td>-0.138956</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>typea</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.087214</td>\n",
       "      <td>0.003846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>typea</td>\n",
       "      <td>NEG</td>\n",
       "      <td>-0.056407</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>obesity</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.084626</td>\n",
       "      <td>0.003846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>obesity</td>\n",
       "      <td>NEG</td>\n",
       "      <td>-0.054733</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.052867</td>\n",
       "      <td>0.003846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>alcohol</td>\n",
       "      <td>NEG</td>\n",
       "      <td>-0.034192</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>age</td>\n",
       "      <td>POS</td>\n",
       "      <td>0.315332</td>\n",
       "      <td>0.003846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>age</td>\n",
       "      <td>NEG</td>\n",
       "      <td>-0.203946</td>\n",
       "      <td>0.002488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          key response  posterior_mean  posterior_variance\n",
       "0         sbp      POS        0.162627            0.003846\n",
       "1         sbp      NEG       -0.105181            0.002488\n",
       "2     tobacco      POS        0.253397            0.003846\n",
       "3     tobacco      NEG       -0.163889            0.002488\n",
       "4         ldl      POS        0.222399            0.003846\n",
       "5         ldl      NEG       -0.143840            0.002488\n",
       "6   adiposity      POS        0.214848            0.003846\n",
       "7   adiposity      NEG       -0.138956            0.002488\n",
       "8       typea      POS        0.087214            0.003846\n",
       "9       typea      NEG       -0.056407            0.002488\n",
       "10    obesity      POS        0.084626            0.003846\n",
       "11    obesity      NEG       -0.054733            0.002488\n",
       "12    alcohol      POS        0.052867            0.003846\n",
       "13    alcohol      NEG       -0.034192            0.002488\n",
       "14        age      POS        0.315332            0.003846\n",
       "15        age      NEG       -0.203946            0.002488"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['key', 'response', 'posterior_mean', 'posterior_variance']\n",
    "compare = []\n",
    "for key in CHD.keys()[0:8]:\n",
    "    pos_x = CHD_POS[key]\n",
    "    pos_posterior_mean, pos_posterior_var= get_estimate(pos_x)\n",
    "    neg_x = CHD_NEG[key]\n",
    "    neg_posterior_mean, neg_posterior_var= get_estimate(neg_x)\n",
    "    compare.append([key, \"POS\", pos_posterior_mean, pos_posterior_var])\n",
    "    compare.append([key, \"NEG\", neg_posterior_mean, neg_posterior_var])\n",
    "\n",
    "df = pd.DataFrame(compare, columns=cols)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response\n",
    "What I found most interesting is that the posterior means for all cases where the patient was CHD positive are positive, where the posterior means for where the patient did not have CHD are negative. The posterior means are somewhat similar in absolute value. \n",
    "The variance is slighly smaller for the patients who are negative, which makes sense because the sample size is slightly larger. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6\n",
    "For each of the following types of distributions, state the support type\n",
    "(single or multivariable and discrete or continuous), the formula for the PMF or\n",
    "PDF, the parameters, the support, the mean, and some typical uses of the\n",
    "distribution. You may use whatever source(s) you want, including for example\n",
    "Wikipedia.\n",
    "\n",
    "## Bernoulli Distribution\n",
    "\n",
    "![](Slide2.jpg)\n",
    "\n",
    "## Binomial Distribution\n",
    "\n",
    "![](Slide3.jpg)\n",
    "\n",
    "## Poisson Distribution\n",
    "\n",
    "![](Slide4.JPG)\n",
    "\n",
    "## Uniform Distribution\n",
    "\n",
    "![](Slide5.JPG)\n",
    "\n",
    "## Beta Distribution\n",
    "\n",
    "![](Slide6.JPG)\n",
    "\n",
    "## Gamma Distribution\n",
    "\n",
    "![](Slide7.JPG)\n",
    "\n",
    "## Gaussian Distribution\n",
    "![](Slide8.JPG)\n",
    "\n",
    "## t Distribution\n",
    "\n",
    "![](Slide9.JPG)\n",
    "\n",
    "## Cauchy Distribution\n",
    "\n",
    "![](Slide10.JPG)\n",
    "\n",
    "## Multinomial Distribution\n",
    "\n",
    "![](Slide11.JPG)\n",
    "\n",
    "## Dirichlet Distribution\n",
    "\n",
    "![](Slide12.JPG)\n",
    "\n",
    "## Multivariate Gaussian Distribution\n",
    "\n",
    "![](Slide13.JPG)\n",
    "\n",
    "## Multivariate t Distribution\n",
    "\n",
    "![](Slide14.JPG)\n",
    "\n",
    "## Wishart Distribution\n",
    "\n",
    "![](Slide15.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. \n",
    "(15) Using the Python Notebook https://www.kaggle.com/billbasener/pt2-\n",
    "probabilities-likelihoods-and-bayes-theorem, complete the challenge question\n",
    "from Section 6: Modify the code from Section 5 to and add the ability to use the\n",
    "posterior from conjugate prior function to output the posterior probability\n",
    "parameters given parameters and for a Gaussian Likelihood with known variance\n",
    "σ2, and use your modified function to create the Prior, Likelihood, Posterior plots\n",
    "as in Section 5 of the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "619226f811c1a3cf24b57725644a3f9927ee4a52cb448157cb29be04a2ef885e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
